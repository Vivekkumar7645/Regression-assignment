{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. What is Simple Linear Regression?\n",
        "**Ans** - Simple Linear Regression is a statistical method used to model the relationship between two variables by fitting a straight line to the data. It predicts the dependent variable (Y) based on the independent variable (X) using the equation:\n",
        "\n",
        "    Y = mX + c\n",
        "where:\n",
        "* Y = Dependent variable\n",
        "* X = Independent variable\n",
        "* m = Slope of the line\n",
        "* c = Intercept\n",
        "\n",
        "**Assumptions of Simple Linear Regression**\n",
        "1. Linearity: The relationship between X and Y is linear.\n",
        "2. Independence: The observations are independent of each other.\n",
        "3. Homoscedasticity: The variance of residuals is constant.\n",
        "4. Normality: The residuals follow a normal distribution.\n",
        "\n",
        "**Use of Simple Linear Regression**\n",
        "* Predicting sales based on advertising expenses\n",
        "* Estimating house prices based on square footage\n",
        "* Forecasting temperature based on altitude"
      ],
      "metadata": {
        "id": "P1D2IIJfgVmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. What are the key assumptions of Simple Linear Regression?\n",
        "**Ans** - The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "**1. Linearity**\n",
        "* The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "* This can be checked using scatter plots or residual plots.\n",
        "\n",
        "**2. Independence**\n",
        "* The observations should be independent of each other.\n",
        "* In time-series data, this means no autocorrelation.\n",
        "\n",
        "**3. Homoscedasticity**\n",
        "* The variance of residuals should remain constant across all values of X.\n",
        "* If variance increases or decreases with X, it leads to heteroscedasticity, which can be checked using residual plots.\n",
        "\n",
        "**4. Normality of Residuals**\n",
        "* The residuals should be normally distributed.\n",
        "* This can be tested using histograms, Q-Q plots, or statistical tests like the Shapiro-Wilk test.\n",
        "\n",
        "**5. No Multicollinearity**\n",
        "* While not a concern in Simple Linear Regression, in Multiple Linear Regression, independent variables should not be highly correlated.\n",
        "* This is tested using Variance Inflation Factor (VIF)."
      ],
      "metadata": {
        "id": "T3ioPV3EgZQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "**Ans** - In the equation of Simple Linear Regression:\n",
        "\n",
        "    Y = mX + c\n",
        "the coefficient 'm' represents the slope of the regression line. It indicates the rate of change of the dependent variable (Y) with respect to the independent variable (X).\n",
        "\n",
        "**Interpretation of m**\n",
        "* 'm' tells us how Y changes for a one-unit increase in X.\n",
        "* If 'm' is positive, Y increases as X increases.\n",
        "* If 'm' is negative, Y decreases as X increases.\n",
        "* If 'm = 0', Y does not change with X.\n",
        "\n",
        "**Example**\n",
        "\n",
        "If the equation is:\n",
        "\n",
        "    Salary = 5000*(Years of Experience) + 30000\n",
        "* Here, m = 5000, meaning for every additional year of experience, salary increases by 5000."
      ],
      "metadata": {
        "id": "ujl204MtgbVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. What does the intercept c represent in the equation Y = mX+c ?\n",
        "**Ans** - In the equation of Simple Linear Regression:\n",
        "\n",
        "    Y = mX + c\n",
        "the intercept c represents the value of Y when X = 0.\n",
        "\n",
        "**Interpretation of c**\n",
        "* It is the point where the regression line crosses the Y-axis.\n",
        "* It shows the expected value of Y when there is no influence from X.\n",
        "* In real-world scenarios, the intercept may or may not have practical significance. Sometimes, an intercept might not make sense.\n",
        "\n",
        "**Example**\n",
        "\n",
        "If the equation is:\n",
        "\n",
        "    Salary = 5000*(Years of Experience) + 30000\n",
        "* Here, c = 30000, meaning a person with 0 years of experience is expected to earn 30,000."
      ],
      "metadata": {
        "id": "ZpD9A7vqO1lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "**Ans** - In Simple Linear Regression, the slope m is calculated using the Least Squares Method, which minimizes the difference between the actual and predicted values.\n",
        "\n",
        "**Formula for m (Slope)**\n",
        "    \n",
        "    m = [∑(Xᵢ-X̄)(Yᵢ-Ȳ)]/[∑(Xᵢ-X̄)²]\n",
        "\n",
        "where:\n",
        "* Xᵢ and Yᵢ are individual data points,\n",
        "* X̄ and Ȳ are the mean of X and Y,\n",
        "* The numerator is the covariance between X and Y,\n",
        "* The denominator is the variance of X.\n",
        "\n",
        "**Step-by-Step Calculation**\n",
        "1. Compute the mean of X and Y.\n",
        "2. Calculate the numerator: sum of the product of deviations of X and Y from their means.\n",
        "3. Calculate the denominator: sum of squared deviations of X from its mean.\n",
        "4. Divide the numerator by the denominator to get m."
      ],
      "metadata": {
        "id": "phy_LWVFO3Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "Y = np.array([2, 3, 5, 4, 6])\n",
        "\n",
        "X_mean = np.mean(X)\n",
        "Y_mean = np.mean(Y)\n",
        "\n",
        "numerator = np.sum((X - X_mean) * (Y - Y_mean))\n",
        "denominator = np.sum((X - X_mean) ** 2)\n",
        "\n",
        "m = numerator / denominator\n",
        "print(f\"Slope (m): {m}\")"
      ],
      "metadata": {
        "id": "eFNgw5vzbWGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "**Ans** - The Least Squares Method in Simple Linear Regression is used to find the best-fitting straight line by minimizing the sum of the squared differences between actual and predicted values.\n",
        "\n",
        "**Purpose of the Least Squares Method**\n",
        "1. Minimizing Error: It ensures that the total error is as small as possible.\n",
        "2. Finding the Best Fit Line: It calculates the optimal values of the slope m and intercept c to fit the data.\n",
        "3. Reducing Negative Error Cancellation: Squaring the errors prevents positive and negative errors from canceling each other out.\n",
        "\n",
        "**Mathematical Explanation**\n",
        "\n",
        "The goal is to minimize the Sum of Squared Errors (SSE):\n",
        "\n",
        "    SSE = ∑(Yᵢ - Ŷᵢ)²\n",
        "\n",
        "where:\n",
        "\n",
        "* Yᵢ = Actual value\n",
        "* Ŷᵢ = mXᵢ + c (Predicted value)\n",
        "* (Yᵢ-Ŷᵢ) = Residual (error)\n",
        "\n",
        "By taking the derivative of SSE with respect to m and c, and setting them to zero, we derive the Least Squares Estimators:\n",
        "\n",
        "    m = [∑(Xᵢ-X̄)(Yᵢ-Ȳ)]/[∑(Xᵢ-X̄)²]\n",
        "    c = Ȳ- mX̄\n",
        "\n",
        "* It provides an unbiased and efficient way to estimate regression coefficients.\n",
        "* It is computationally simple and widely used in regression analysis.\n",
        "* It works well under normality and homoscedasticity assumptions."
      ],
      "metadata": {
        "id": "WFNOU5vDO4pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "**Ans** - The coefficient of determination (R²) in Simple Linear Regression measures how well the regression line explains the variability of the dependent variable (Y). It indicates the goodness of fit of the model.\n",
        "\n",
        "**Formula for R²**\n",
        "\n",
        "    R² = 1-(SSᵣₑₛ/SSₜₒₜ)\n",
        "where:\n",
        "* SSᵣₑₛ = ∑(Yᵢ-Ŷᵢ)² - Residual Sum of Squares (SSE)\n",
        "* SSₜₒₜ = ∑(Yᵢ-Ȳ)² - Total Sum of Squares (SST)\n",
        "* Yᵢ = Actual values\n",
        "* Ŷᵢ = Predicted values\n",
        "* Ȳ = Mean of Y\n",
        "\n",
        "**Interpretation of R²**\n",
        "* R² = 1 - The model perfectly explains all the variance in Y.\n",
        "* R² = 0 - The model does not explain any variance in Y.\n",
        "* Higher R² values (close to 1) indicate a better fit.\n",
        "* Lower R² values (close to 0) suggest that the model does not explain much of the variation in Y.\n",
        "\n",
        "**Example Interpretations**\n",
        "* If R² = 0.85, the model explains 85% of the variance in Y, meaning it fits the data well.\n",
        "* If R² = 0.20, the model explains only 20% of the variance, suggesting a poor fit."
      ],
      "metadata": {
        "id": "GH81aAGeO56b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "Y_actual = np.array([2, 3, 5, 4, 6])\n",
        "Y_predicted = np.array([2.2, 3.1, 4.8, 4.2, 5.9])\n",
        "\n",
        "SST = np.sum((Y_actual - np.mean(Y_actual)) ** 2)\n",
        "SSE = np.sum((Y_actual - Y_predicted) ** 2)\n",
        "\n",
        "R_squared = 1 - (SSE / SST)\n",
        "print(f\"R² Score: {R_squared}\")"
      ],
      "metadata": {
        "id": "dm3QmuvvL8KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. What is Multiple Linear Regression?\n",
        "**Ans** - **Multiple Linear Regression (MLR)**\n",
        "\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression, where we use multiple independent variables (X₁,X₂,X₃,.....) to predict a dependent variable (Y).\n",
        "\n",
        "**Equation of Multiple Linear Regression**\n",
        "\n",
        "    Y = b₀+b₁X₁+b₂X₂+...+ bₙXₙ+ϵ\n",
        "where:\n",
        "* Y = Dependent variable\n",
        "* b₀ = Intercept\n",
        "* b₁,b₂,...,bₙ = Regression coefficients\n",
        "* X₁,X₂,...,Xₙ = Independent variables\n",
        "* ϵ = Error term\n",
        "\n",
        "**Assumptions of Multiple Linear Regression**\n",
        "1. Linearity: The relationship between each independent variable and Y is linear.\n",
        "2. Independence: The observations are independent of each other.\n",
        "3. Homoscedasticity: The variance of residuals is constant across all values of X.\n",
        "4. Normality of Residuals: Residuals should follow a normal distribution.\n",
        "5. No Multicollinearity: Independent variables should not be highly correlated.\n",
        "\n",
        "**Example Use Cases**\n",
        "* Predicting house prices based on square footage, number of bedrooms, and location.\n",
        "* Estimating sales revenue using advertising spend across multiple channels.\n",
        "* Forecasting employee salaries based on experience, education level, and job role.\n",
        "\n",
        "**Python Implementation**\n",
        "\n",
        "perform Multiple Linear Regression using Scikit-Learn:"
      ],
      "metadata": {
        "id": "DOPJz_VZO75u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Experience': [1, 2, 3, 4, 5],\n",
        "    'Education': [12, 14, 16, 18, 20],\n",
        "    'Salary': [30000, 35000, 40000, 45000, 50000]\n",
        "})\n",
        "\n",
        "X = data[['Experience', 'Education']]\n",
        "Y = data['Salary']\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "\n",
        "prediction = model.predict([[6, 22]])\n",
        "print(f\"Predicted Salary: {prediction[0]}\")"
      ],
      "metadata": {
        "id": "dwnNhxTqOn9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "**Ans** - The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "|Feature\t|Simple Linear Regression\t|Multiple Linear Regression|\n",
        "|-|||\n",
        "|Number of Independent Variables\t|One (X)\t|Two or more (X₁,X₂,...,Xₙ)|\n",
        "|Equation\t| Y = mX+c\t|Y = b₀+b₁X₁+b₂X₂+...+bₙXₙ+ϵ|\n",
        "|Interpretation\t| Measures the effect of a single predictor on Y\t|Measures the effect of multiple predictors on Y|\n",
        "|Use Case\t|Predicting salary based on years of experience\t|Predicting salary based on experience, education, and job role|\n",
        "|Complexity\t|Simple and easy to interpret\t|More complex due to multiple variables and possible multicollinearity|\n",
        "\n",
        "**Example**\n",
        "1. Simple Linear Regression\n",
        "  * Predicting house price based only on square footage.\n",
        "\n",
        "          Price=500*(Square Footage) + 20000\n",
        "\n",
        "2. Multiple Linear Regression\n",
        "* Predicting house price based on square footage, number of bedrooms, and location.\n",
        "\n",
        "      Price = 500*(Square Footage) + 10000*(Bedrooms) + 15000*(Location Score) + 20000"
      ],
      "metadata": {
        "id": "12-k3oWoO8Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. What are the key assumptions of Multiple Linear Regression?\n",
        "**Ans** - **Assumptions of Multiple Linear Regression**\n",
        "\n",
        "Multiple Linear Regression relies on several key assumptions to ensure the validity of the model. These include:\n",
        "\n",
        "**1. Linearity**\n",
        "* The relationship between the independent variables (X₁,X₂,...) and the dependent variable (Y) must be linear.\n",
        "* Checked using scatter plots and residual plots.\n",
        "\n",
        "**2. Independence**\n",
        "* Observations should be independent of each other.\n",
        "* In time-series data, there should be no autocorrelation.\n",
        "\n",
        "**3. Homoscedasticity**\n",
        "* The variance of residuals should remain constant across all values of X.\n",
        "* Checked using a residual plot.\n",
        "\n",
        "**4. Normality of Residuals**\n",
        "* Residuals should follow a normal distribution.\n",
        "* Checked using histograms, Q-Q plots, or the Shapiro-Wilk test.\n",
        "\n",
        "**5. No Multicollinearity**\n",
        "* Independent variables should not be highly correlated with each other.\n",
        "* Checked using Variance Inflation Factor (VIF):\n",
        "  * VIF > 10 indicates high multicollinearity.\n",
        "  * VIF < 5 is acceptable.\n",
        "\n",
        "**6. No Omitted Variable Bias**\n",
        "* All important independent variables should be included in the model.\n",
        "* Omitting significant predictors can bias the estimates of included variables."
      ],
      "metadata": {
        "id": "1H3YJJjhO8lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Experience': [1, 2, 3, 4, 5],\n",
        "    'Education': [12, 14, 16, 18, 20],\n",
        "    'Salary': [30000, 35000, 40000, 45000, 50000]\n",
        "})\n",
        "\n",
        "X = data[['Experience', 'Education']]\n",
        "X = sm.add_constant(X)\n",
        "Y = data['Salary']\n",
        "\n",
        "model = sm.OLS(Y, X).fit()\n",
        "\n",
        "import scipy.stats as stats\n",
        "residuals = model.resid\n",
        "print(\"Shapiro-Wilk Test p-value:\", stats.shapiro(residuals)[1])\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)\n",
        "\n",
        "print(\"Durbin-Watson Test:\", sm.stats.stattools.durbin_watson(residuals))"
      ],
      "metadata": {
        "id": "Be_Z2cG6Yeeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "**Ans** - Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variables in a regression model.\n",
        "* In a well-behaved regression model, errors should be homoscedastic.\n",
        "* Heteroscedasticity means that as the values of X increase or decrease, the spread of errors increases or decreases unevenly.\n",
        "\n",
        "**Visual Representation**\n",
        "* A residual plot of a homoscedastic model shows a random scatter of points.\n",
        "* A residual plot of a heteroscedastic model shows a funnel shape, where variance increases as X increases.\n",
        "\n",
        "**Effects of Heteroscedasticity on Multiple Linear Regression**\n",
        "1. Biased Standard Errors\n",
        "* It does not bias the regression coefficients (b₀,b₁,b₂,....) themselves.\n",
        "* However, it makes standard errors unreliable, leading to incorrect hypothesis test results.\n",
        "\n",
        "2. Inaccurate Confidence Intervals and Hypothesis Testing\n",
        "* Since standard errors are incorrect, t-tests and p-values may be misleading.\n",
        "* This can result in false conclusions about whether predictors are statistically significant.\n",
        "\n",
        "3. Inefficient Estimates\n",
        "* Ordinary Least Squares assumes homoscedasticity to provide the best linear unbiased estimates.\n",
        "* If heteroscedasticity is present, the estimates remain unbiased but are not efficient.\n",
        "\n",
        "How to Detect Heteroscedasticity?\n",
        "* Residual Plot: Scatter plot of residuals vs. predicted values should show random distribution.\n",
        "* Breusch-Pagan Test: A statistical test to detect heteroscedasticity.\n",
        "* White's Test: Another robust test for heteroscedasticity.\n",
        "\n",
        "**Python Code to Detect Heteroscedasticity**"
      ],
      "metadata": {
        "id": "2-YFECiBO84I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Experience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'Salary': [30000, 32000, 35000, 37000, 45000, 52000, 60000, 75000, 90000, 110000]\n",
        "})\n",
        "\n",
        "X = data[['Experience']]\n",
        "X = sm.add_constant(X)\n",
        "Y = data['Salary']\n",
        "\n",
        "model = sm.OLS(Y, X).fit()\n",
        "\n",
        "plt.scatter(model.fittedvalues, model.resid)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Fitted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot for Heteroscedasticity Check')\n",
        "plt.show()\n",
        "\n",
        "bp_test = het_breuschpagan(model.resid, X)\n",
        "print(f\"Breusch-Pagan Test p-value: {bp_test[1]}\")"
      ],
      "metadata": {
        "id": "zjx3BeU1aIJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Fix Heteroscedasticity?\n",
        "1. Use Weighted Least Squares\n",
        "* Instead of OLS, WLS assigns weights to observations based on error variance.\n",
        "2. Transform Variables\n",
        "* Apply log transformation to the dependent variable (Y) to stabilize variance.\n",
        "* Example: Use log(Y) instead of Y.\n",
        "3. Use Robust Standard Errors\n",
        "* Adjust standard errors using Heteroscedasticity-Consistent standard errors."
      ],
      "metadata": {
        "id": "o03OVyRaac20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_robust = model.get_robustcov_results()\n",
        "print(model_robust.summary())"
      ],
      "metadata": {
        "id": "VCoL9pHFa0XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "**Ans** - Multicollinearity occurs when two or more independent variables are highly correlated, leading to unreliable coefficient estimates. High multicollinearity increases standard errors, making it difficult to determine the true effect of each predictor.\n",
        "\n",
        "**Step 1: Detect Multicollinearity**\n",
        "* Variance Inflation Factor (VIF): A VIF > 10 suggests severe multicollinearity.\n",
        "* Correlation Matrix: Check pairwise correlations (values > 0.8 indicate potential issues)."
      ],
      "metadata": {
        "id": "HAZ-tUboO9GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Experience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'Education': [10, 12, 14, 16, 18, 20, 22, 24, 26, 28],\n",
        "    'Salary': [30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000]\n",
        "})\n",
        "\n",
        "X = data[['Experience', 'Education']]\n",
        "X = sm.add_constant(X)\n",
        "Y = data['Salary']\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "RvJS_quJbi5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Fix Multicollinearity**\n",
        "1. Remove One of the Highly Correlated Variables\n",
        "* If two variables are highly correlated, drop one."
      ],
      "metadata": {
        "id": "4I2nt1uabyhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Experience']]\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(Y, X).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "vZK0uOxbcDvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Use Principal Component Analysis (PCA)\n",
        "* PCA reduces dimensionality by creating uncorrelated components from correlated features."
      ],
      "metadata": {
        "id": "sXLtIjbvcIZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=1)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "model_pca = sm.OLS(Y, sm.add_constant(X_pca)).fit()\n",
        "print(model_pca.summary())"
      ],
      "metadata": {
        "id": "-gMOepTUcXjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use Ridge or Lasso Regression\n",
        "* Ridge Regression: Shrinks coefficients but keeps all variables.\n",
        "* Lasso Regression: Shrinks some coefficients to zero, effectively selecting important variables."
      ],
      "metadata": {
        "id": "jiNIUxlJcduq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X, Y)\n",
        "print(f\"Ridge Coefficients: {ridge.coef_}\")\n",
        "\n",
        "lasso = Lasso(alpha=1.0)\n",
        "lasso.fit(X, Y)\n",
        "print(f\"Lasso Coefficients: {lasso.coef_}\")"
      ],
      "metadata": {
        "id": "qWwhczxUcn0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create Interaction or Composite Variables\n",
        "* Instead of using two correlated variables separately, combine them into a single meaningful variable."
      ],
      "metadata": {
        "id": "mwBWm6GvcsNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['Exp_Edu_Composite'] = data['Experience'] * data['Education']\n",
        "X = data[['Exp_Edu_Composite']]\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(Y, X).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "rY4p9Do_czrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Collect More Data\n",
        "* If possible, increase sample size to reduce multicollinearity's impact.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "|Method\t|When to Use?|\n",
        "|-||\n",
        "|Drop a Variable\t|If two variables are highly correlated and one is redundant.|\n",
        "|PCA\t|When you want to retain all features but remove correlation.|\n",
        "|Ridge Regression\t|If all features are important but you want to reduce multicollinearity.|\n",
        "|Lasso Regression\t|If you want to automatically remove less important variables.|\n",
        "|Create Composite Features\t|When correlated variables have a meaningful interaction.|"
      ],
      "metadata": {
        "id": "lsCwooMYc2Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "**Ans** - **Common Techniques for Transforming Categorical Variables in Regression Models**\n",
        "\n",
        "Since regression models require numerical inputs, categorical variables must be converted into numerical form. Below are the most commonly used methods:\n",
        "\n",
        "**1. One-Hot Encoding**\n",
        "* Best for nominal categories.\n",
        "* Creates binary (0/1) columns for each category.\n",
        "* Can increase dimensionality if there are many categories.\n",
        "\n",
        "**Example**\n",
        "\n",
        "|City\t|One-Hot Encoding|\n",
        "|-||\n",
        "|New York\t|(1,0,0)|\n",
        "|Los Angeles\t|(0,1,0)|\n",
        "|Chicago\t|(0,0,1)|"
      ],
      "metadata": {
        "id": "XuDOEsR6O9aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'City': ['New York', 'Los Angeles', 'Chicago']})\n",
        "one_hot = pd.get_dummies(data, columns=['City'], drop_first=True)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "id": "svs7p8DWd7Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Label Encoding**\n",
        "* Assigns integer values to each category.\n",
        "* Works well for ordinal categories.\n",
        "* Not ideal for nominal data because it introduces artificial order.\n",
        "\n",
        "**Example**\n",
        "\n",
        "|Size\t|Label Encoding|\n",
        "|-||\n",
        "|Small\t|0|\n",
        "|Medium\t|1|\n",
        "|Large\t|2|"
      ],
      "metadata": {
        "id": "R2rwKJAreC20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.DataFrame({'Size': ['Small', 'Medium', 'Large']})\n",
        "le = LabelEncoder()\n",
        "data['Size_encoded'] = le.fit_transform(data['Size'])\n",
        "print(data)"
      ],
      "metadata": {
        "id": "PHKnahIbesgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Ordinal Encoding (When Order Matters)**\n",
        "* Works well for ordered categories (e.g., Education Level: High School < Bachelor's < Master's).\n",
        "* Should not be used for nominal data like \"City\" or \"Color\".\n",
        "\n",
        "**Example**\n",
        "\n",
        "|Education Level\t|Ordinal Encoding|\n",
        "|-||\n",
        "|High School\t|1|\n",
        "|Bachelor’s\t|2|\n",
        "|Master’s\t|3|"
      ],
      "metadata": {
        "id": "hLdaJ61mezSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = pd.DataFrame({'Education': ['High School', 'Bachelor’s', 'Master’s']})\n",
        "ordinal = OrdinalEncoder(categories=[['High School', 'Bachelor’s', 'Master’s']])\n",
        "data['Education_encoded'] = ordinal.fit_transform(data[['Education']])\n",
        "print(data)"
      ],
      "metadata": {
        "id": "4lC-8YRbfSGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Target Encoding**\n",
        "* Replaces categories with mean of target variable (Y).\n",
        "* Useful when there are many categories.\n",
        "* Can lead to overfitting.\n",
        "\n",
        "**Example (Predicting House Prices)**\n",
        "\n",
        "|Neighborhood\t|Average House Price ($)|\n",
        "|-||\n",
        "|A\t|500,000|\n",
        "|B\t|400,000|\n",
        "|C\t|600,000|"
      ],
      "metadata": {
        "id": "HRivXh0wfWAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({'Neighborhood': ['A', 'B', 'C'], 'HousePrice': [500000, 400000, 600000]})\n",
        "data['Neighborhood_encoded'] = data.groupby('Neighborhood')['HousePrice'].transform('mean')\n",
        "print(data)"
      ],
      "metadata": {
        "id": "v-YS3Ukyg6aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Frequency Encoding**\n",
        "* Replaces categories with their occurrence count.\n",
        "* Useful for high-cardinality categorical features.\n",
        "* Ignores relationships between categories and the target variable.\n",
        "\n",
        "**Example**\n",
        "\n",
        "|City\t|Count\t|Frequency Encoding|\n",
        "|-|||\n",
        "|New York\t|50\t|50|\n",
        "|Los Angeles\t|30\t|30|\n",
        "|Chicago\t|20\t|20|"
      ],
      "metadata": {
        "id": "o1WkcloDg9fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({'City': ['New York', 'Los Angeles', 'Chicago', 'New York']})\n",
        "data['City_encoded'] = data['City'].map(data['City'].value_counts())\n",
        "print(data)"
      ],
      "metadata": {
        "id": "RQMM0iIVhcQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Binary Encoding**\n",
        "* Converts categories into binary format and splits them into separate columns.\n",
        "* Reduces dimensionality compared to one-hot encoding.\n",
        "\n",
        "**Example**\n",
        "\n",
        "|Category\t|Binary Representation\t|Binary Encoding|\n",
        "|-|||\n",
        "|A (0)\t|00\t|(0,0)|\n",
        "|B (1)\t|01\t|(0,1)|\n",
        "|C (2)\t|10\t|(1,0)|\n",
        "|D (3)\t|11\t|(1,1)|"
      ],
      "metadata": {
        "id": "c5VFcrzthfo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import category_encoders as ce\n",
        "\n",
        "data = pd.DataFrame({'Category': ['A', 'B', 'C', 'D']})\n",
        "encoder = ce.BinaryEncoder(cols=['Category'])\n",
        "data_encoded = encoder.fit_transform(data)\n",
        "print(data_encoded)"
      ],
      "metadata": {
        "id": "seONr8l0iA8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing the Right Encoding Method**\n",
        "\n",
        "|Encoding Method\t|When to Use?|\n",
        "|-||\n",
        "|One-Hot Encoding\t|When categories are nominal (unordered, few categories).|\n",
        "|Label Encoding\t|When categories are ordinal (have a natural order).|\n",
        "|Ordinal Encoding\t|When categories have a clear ranking (e.g., Education Level).|\n",
        "|Target Encoding\t|When categories are numerous, and target mean is meaningful.|\n",
        "|Frequency Encoding\t|When categories are high-cardinality and their count matters.|\n",
        "|Binary Encoding\t|When one-hot encoding is too high-dimensional.|"
      ],
      "metadata": {
        "id": "vEZ82J-OiEZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "**Ans** - **Role of Interaction Terms in Multiple Linear Regression**\n",
        "1. Definition - Interaction terms capture the combined effect of two or more independent variables on the dependent variable. They help model relationships that are not simply additive.\n",
        "\n",
        "In standard multiple linear regression:\n",
        "\n",
        "    Y = β₀ + β₁X₁ + β₂X₂ + ϵ\n",
        "the effect of X₁ and X₂ on Y is assumed to be independent. However, if the impact of X₁ depends on X₂, an interaction term (X₁*X₂) should be included:\n",
        "\n",
        "    Y = β₀ + β₁X₁ + β₂X₂ + β₃(X₁*X₂) + ϵ\n",
        "where β₃ represents the interaction effect.\n",
        "\n",
        "**2. Interaction Terms are Important**\n",
        "* Capture Real-World Relationships: Some variables influence the outcome differently when combined.\n",
        "* Improve Model Accuracy: They help explain variance that simple linear terms cannot.\n",
        "* Avoid Misleading Interpretations: Without interactions, regression may underestimate or miss important effects.\n",
        "\n",
        "**3. Use Case**\n",
        "\n",
        "Scenario: Predicting Salary Based on Experience & Education\n",
        "* X₁ = Years of Experience\n",
        "* X₂ = Education Level (e.g., Bachelor's, Master's, Ph.D.)\n",
        "* Y = Salary\n",
        "\n",
        "Without interaction:\n",
        "\n",
        "    Salary = β₀ + β₁ Experience + β₂ Education+ϵ\n",
        "This assumes that each additional year of experience always increases salary by the same amount, regardless of education level.\n",
        "\n",
        "With interaction:\n",
        "\n",
        "    Salary = β₀ + β₁ Experience + β₂ Education + β₃(Experience*Education) + ϵ\n",
        "* If β₃ > 0, the effect of experience on salary is stronger for those with higher education.\n",
        "* If β₃ < 0, higher education reduces the impact of experience on salary.\n",
        "\n",
        "**4. Implementing Interaction Terms in Python**"
      ],
      "metadata": {
        "id": "gYDCfjcUO9rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Experience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'Education': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "    'Salary': [40000, 50000, 42000, 55000, 45000, 60000, 48000, 65000, 51000, 70000]\n",
        "})\n",
        "\n",
        "data['Experience_Education'] = data['Experience'] * data['Education']\n",
        "\n",
        "X = data[['Experience', 'Education', 'Experience_Education']]\n",
        "X = sm.add_constant(X)\n",
        "Y = data['Salary']\n",
        "\n",
        "model = sm.OLS(Y, X).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "YTKSI6yllvbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Interpreting Results**\n",
        "* β₁(Experience) - Effect of experience on salary when education = 0.\n",
        "* β₂(Education) - Effect of education on salary when experience = 0.\n",
        "* β₃(Interaction Term) - How education changes the effect of experience on salary.\n",
        "\n",
        "If β₃ is significant, experience and education interact, meaning their combined effect is different from their separate effects.\n",
        "\n",
        "**6. Use of Interaction Terms**\n",
        "* When two predictors affect the outcome in a non-additive way.\n",
        "* When we suspect one variable modifies the impact of another.\n",
        "* When theoretical or domain knowledge suggests interactions matter.\n",
        "* Avoid blindly adding interactions without proper interpretation—this can lead to overfitting."
      ],
      "metadata": {
        "id": "WU3d0SuCl8_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "**Ans** - **Interpretation of the Intercept in Simple vs. Multiple Linear Regression**\n",
        "\n",
        "The intercept (β₀) in a regression equation represents the predicted value of the dependent variable (Y) when all independent variables (X) are zero.\n",
        "\n",
        "**Intercept in Simple Linear Regression**\n",
        "Equation:\n",
        "\n",
        "        Y = β₀+β₁X+ϵ\n",
        "* β₀ represents the expected value of Y when X = 0.\n",
        "* Example: Predicting salary based on experience:\n",
        "\n",
        "      Salary = 30000 + 5000*Experience\n",
        "* β₀ = 30,000 - This means that when Experience = 0 years, the expected salary is 30,000.\n",
        "* Interpretation makes sense if X = 0 is meaningful (e.g., 0 years of experience exists).\n",
        "* But: If X = 0 is not realistic (e.g., predicting weight based on height, where height = 0 is impossible), the intercept loses real-world meaning.\n",
        "\n",
        "**Intercept in Multiple Linear Regression**\n",
        "Equation:\n",
        "\n",
        "    Y = β₀ + β₁X₁ + β₂X₂ +...+ βₙXₙ + ϵ\n",
        "* β₀ is the predicted value of Y when all X's are zero.\n",
        "* This interpretation is often less meaningful in multiple regression because having all predictors at zero may not be realistic.\n",
        "\n",
        "* Example: Predicting house prices based on size (X₁) and number of bedrooms (X₂):\n",
        "\n",
        "      Price = 50000 + 150*Size + 10000*Bedrooms\n",
        "* β₀ = 50,000 suggests that if both Size = 0 and Bedrooms = 0, the house price is $50,000.\n",
        "* But a house with 0 size and 0 bedrooms doesn't exist, so β₀ is just a model coefficient with no practical meaning.\n",
        "\n",
        "**Differences**\n",
        "\n",
        "|Feature\t|Simple Linear Regression\t|Multiple Linear Regression|\n",
        "|-|||\n",
        "|Definition\t|Value of Y when X=0\t|Value of Y when all X's are 0|\n",
        "|Interpretation\t|Often meaningful\t|Often unrealistic|\n",
        "|Example (Salary vs. Experience)\t|Salary when experience = 0\t|Salary when experience = 0 and education = 0, which may not make sense|\n",
        "|When it’s useful?\t|When X=0 is realistic\t|If zero values for all predictors make sense|"
      ],
      "metadata": {
        "id": "jNqyU974O_-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "**Ans** - **Significance of the Slope in Regression Analysis & Its Effect on Predictions**\n",
        "\n",
        "The slope (β₁) in a regression equation represents the rate of change in the dependent variable (Y) for a one-unit increase in the independent variable (X), assuming all other variables remain constant.\n",
        "\n",
        "For Simple Linear Regression:\n",
        "\n",
        "    Y = β₀ + β₁X + ϵ\n",
        "* β₁ (Slope): Measures the change in Y for each 1-unit increase in X.\n",
        "\n",
        "For Multiple Linear Regression:\n",
        "\n",
        "    Y = β₀ + β₁X₁ + β₂X₂ +...+ βₙXₙ + ϵ\n",
        "* Each βₙ represents the effect of that variable while holding others constant.\n",
        "\n",
        "**Slope Affect Predictions**\n",
        "\n",
        "The slope determines the strength and direction of the relationship between X and Y:\n",
        "* β₁ > 0 - Positive relationship (as X increases, Y increases).\n",
        "* β₁ < 0 - Negative relationship (as X increases, Y decreases).\n",
        "* β₁ = 0 - No relationship (changing X has no impact on Y).\n",
        "\n",
        "**Example (Salary vs. Experience):**\n",
        "\n",
        "    Salary = 30,000 + 5,000*Experience\n",
        "* β₁ = 5,000 - Each additional year of experience increases salary by $5,000.\n",
        "* If Experience = 10 years:\n",
        "\n",
        "      Salary = 30,000 + (5,000*10) = 80,000\n",
        "\n",
        "**Interpreting the Magnitude of the Slope**\n",
        "* A larger absolute value of β₁ means a stronger effect of X on Y.\n",
        "* A smaller slope means a weaker relationship.\n",
        "\n",
        "**Example (House Prices vs. Square Footage):**\n",
        "* β₁ = 200 - Every additional square foot adds 200 to the house price.\n",
        "* β₁ = 10 - Every additional square foot adds only $10, so size has a much weaker effect.\n",
        "\n",
        "**Statistical Significance of the Slope**\n",
        "\n",
        "To determine if the slope is significant, we check its p-value from the regression output:\n",
        "* p-value < 0.05 - Slope is significant (there is strong evidence that X affects Y).\n",
        "* p-value > 0.05 - Slope is not significant (no strong evidence of an effect).\n",
        "\n",
        "**Python Code to Check Slope Significance**"
      ],
      "metadata": {
        "id": "9X7HgwLm0bkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Experience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                     'Salary': [35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 80000]})\n",
        "\n",
        "X = sm.add_constant(data['Experience'])\n",
        "Y = data['Salary']\n",
        "\n",
        "model = sm.OLS(Y, X).fit()\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "Blh6n1-aSzqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When Is the Slope Misleading**\n",
        "* Outliers can drastically change the slope.\n",
        "* Multicollinearity in multiple regression can make slopes unreliable.\n",
        "* Non-linear relationships may require transformations (e.g., log, polynomial regression)."
      ],
      "metadata": {
        "id": "CmjvsIVjTDhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "**Ans** - **Intercept Provides Context in a Regression Model**\n",
        "\n",
        "The intercept (β₀) in a regression model is the predicted value of the dependent variable (Y) when all independent variables (X) are zero.\n",
        "\n",
        "For Simple Linear Regression:\n",
        "\n",
        "    Y = β₀ + β₁X + ϵ\n",
        "* β₀ represents the value of Y when X = 0.\n",
        "\n",
        "For Multiple Linear Regression:\n",
        "\n",
        "    Y = β₀ + β₁X₁ + β₂X₂ +...+ βₙXₙ + ϵ\n",
        "* β₀ represents the expected value of Y when all X's are 0.\n",
        "\n",
        "**Intercept Provide Context**\n",
        "* Defines a Baseline Value: Represents the starting point of Y before the effects of X variables.\n",
        "* Indicates Realistic vs. Unreasonable Scenarios: If X = 0 makes sense, the intercept is meaningful. Otherwise, it's just a model artifact.\n",
        "* Helps in Predictions: Even if unrealistic,\n",
        " β₀ still ensures the correct scaling of the model.\n",
        "\n",
        "**Interpretation Examples**\n",
        "* Example 1: Predicting Salary Based on Experience\n",
        "\n",
        "      Salary = 30,000 + 5,000*Experience\n",
        "* β₀ = 30,000 - If Experience = 0 years, the expected salary is $30,000.\n",
        "* Interpretation makes sense because entry-level salaries exist.\n",
        "\n",
        "* Example 2: Predicting Weight Based on Height\n",
        "\n",
        "      Weight = -50 + 0.5*Height\n",
        "* β₀ = -50 - When Height = 0 cm, predicted weight is -50 kg, which is not physically meaningful.\n",
        "* The intercept exists only for mathematical reasons and doesn't provide real-world context.\n",
        "\n",
        "* Example 3: Predicting House Price Based on Size & Bedrooms\n",
        "\n",
        "      Price = 50,000 + 200*Size + 10,000*Bedrooms\n",
        "* β₀ = 50,000 - If Size = 0 sqft and Bedrooms = 0, base price is $50,000.\n",
        "* May not be realistic, but helps anchor the model.\n",
        "\n",
        "**When Intercept is Important**\n",
        "* When X = 0 is realistic - Provides meaningful starting values.\n",
        "* When comparing models - Different intercepts show baseline differences.\n",
        "* When controlling for variables - Shows how much\n",
        " Y exists independent of X.\n",
        "\n",
        "**When Intercept is Less Useful**\n",
        "* If X = 0 is impossible (e.g., height, age, area).\n",
        "* If it's outside the data range (e.g., predicting sales when demand is 0)."
      ],
      "metadata": {
        "id": "yRK2QraoPAYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18. What are the limitations of using R² as a sole measure of model performance?\n",
        "**Ans** - **Limitations of Using R² as the Sole Measure of Model Performance**\n",
        "\n",
        "The coefficient of determination (R²) measures how well a regression model explains the variance in the dependent variable (Y). While useful, relying only on R² has several limitations:\n",
        "\n",
        "1. R² Does Not Indicate Model Accuracy\n",
        "* A high R² doesn't mean the model makes accurate predictions.\n",
        "* Example: A model with high R² but large prediction errors is still unreliable.\n",
        "* Solution: Check Root Mean Squared Error or Mean Absolute Error for accuracy.\n",
        "\n",
        "2. R² Cannot Detect Overfitting\n",
        "* Adding more variables always increases R², even if they are irrelevant.\n",
        "* This can lead to overfitting, where the model fits training data well but performs poorly on new data.\n",
        "* Solution: Use Adjusted R², which penalizes unnecessary predictors.\n",
        "\n",
        "3. R² Assumes a Linear Relationship\n",
        "* If the true relationship is non-linear,R² might be misleading.\n",
        "* A low R² doesn't always mean a poor model—it may just mean the relationship isn't linear.\n",
        "* Solution: Try polynomial regression or non-linear models.\n",
        "\n",
        "4. R² Does Not Detect Multicollinearity\n",
        "* High correlation between independent variables can inflate R².\n",
        "* The model may seem strong, but individual predictors might be redundant.\n",
        "* Solution: Check Variance Inflation Factor to detect multicollinearity.\n",
        "\n",
        "5. R² Does Not Apply to All Models\n",
        "* In logistic regression, classification models, or tree-based models,R² is not useful.\n",
        "* Solution: Use AUC-ROC or Adjusted R², AIC, and BIC.\n",
        "\n",
        "6. R² Can Be Misleading with Small Data\n",
        "* In small datasets, R² values can fluctuate significantly.\n",
        "* A low R² may not mean the model is bad—just that there isn't enough data.\n",
        "* Solution: Use cross-validation to test model performance on new data.\n",
        "\n",
        "**Better Alternatives to R² for Model Evaluation**\n",
        "\n",
        "|Metric\t|When to Use|\n",
        "|-||\n",
        "|Adjusted R²\t|When comparing models with different numbers of predictors|\n",
        "|RMSE / MAE\t|When measuring prediction accuracy|\n",
        "|AIC / BIC\t|When choosing between regression models|\n",
        "|VIF\t|To check for multicollinearity|\n",
        "|Cross-validation scores\t|To test model generalization|"
      ],
      "metadata": {
        "id": "DTJRihVrPAwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. How would you interpret a large standard error for a regression coefficient?\n",
        "**Ans** - **Interpreting a Large Standard Error for a Regression Coefficient**\n",
        "\n",
        "In regression analysis, the standard error of a coefficient measures the variability of that coefficient's estimate across different samples. A large standard error indicates that the coefficient estimate is unstable and imprecise.\n",
        "\n",
        "**1. Meaning of a Large Standard Error**\n",
        "\n",
        "If a regression coefficient has a large standard error, it suggests:\n",
        "* High Variability: The coefficient estimate fluctuates significantly across different samples.\n",
        "* Low Confidence: We are less certain about the true effect of that variable.\n",
        "* Potential Insignificance: The predictor may not be strongly related to the dependent variable.\n",
        "\n",
        "Mathematically, the t-statistic is calculated as:\n",
        "\n",
        "    t = (β/SE)\n",
        "* If SE is large, the t-statistic is small, leading to a high p-value.\n",
        "\n",
        "**2. Possible Causes of a Large Standard Error**\n",
        "* Multicollinearity: If predictors are highly correlated, their standard errors increase, making coefficient estimates unreliable.\n",
        "* Small Sample Size: Fewer data points lead to greater uncertainty in coefficient estimates.\n",
        "* High Variance in Residuals: If the model doesn't fit well, residual variance increases, inflating standard errors.\n",
        "* Poor Model Specification: Missing important variables or using irrelevant ones can lead to unstable coefficients.\n",
        "\n",
        "**3. Address a Large Standard Error**\n",
        "* Check for Multicollinearity → Use Variance Inflation Factor and remove redundant predictors.\n",
        "* Increase Sample Size - More data reduces standard error and improves estimate stability.\n",
        "* Improve Model Fit - Add relevant predictors or use non-linear transformations if needed.\n",
        "* Reduce Variability in Data → If possible, control for factors causing extreme variations in residuals.\n",
        "\n",
        "**4. Example Interpretation**\n",
        "\n",
        "Suppose we have the regression equation:\n",
        "\n",
        "    Salary = 30,000 + 5,000*Experience + 2,000*Education\n",
        "If Education has a large standard error, it means its effect on salary is uncertain - a small change in the dataset might give a very different coefficient."
      ],
      "metadata": {
        "id": "jQJUyd9yPBdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "**Ans** - **Identifying Heteroscedasticity in Residual Plots & Why It Matters**\n",
        "\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variable. In other words, the spread of errors changes rather than remaining uniform.\n",
        "\n",
        "**Why is it a problem**\n",
        "* Violates a key assumption of Ordinary Least Squares regression, which assumes homoscedasticity.\n",
        "* Leads to biased standard errors, making hypothesis tests unreliable.\n",
        "\n",
        "**Identify Heteroscedasticity Using Residual Plots**\n",
        "\n",
        "A residual vs. fitted values plot is commonly used to detect heteroscedasticity.\n",
        "* Steps to check heteroscedasticity:\n",
        "1. Plot residuals on the Y-axis.\n",
        "2. Plot fitted values (predicted Y) on the X-axis.\n",
        "3. Look for patterns:\n",
        "* Homoscedasticity (Good) - Residuals are randomly scattered.\n",
        "* Heteroscedasticity (Problem) - Residuals form a funnel shape.\n",
        "\n",
        "**Example Patterns in Residual Plots:**\n",
        "* Cone shape - Increasing or decreasing variance (common sign of heteroscedasticity).\n",
        "* Curved or systematic patterns - Possible non-linearity in data.\n",
        "\n",
        "**Python code for Checking Residuals for Heteroscedasticity**"
      ],
      "metadata": {
        "id": "WQ5R0RnuPB7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.linspace(1, 50, 100)\n",
        "Y = 5 * X + np.random.normal(0, X**0.5, 100)\n",
        "\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(Y, X).fit()\n",
        "residuals = model.resid\n",
        "fitted_values = model.fittedvalues\n",
        "\n",
        "plt.scatter(fitted_values, residuals, alpha=0.7)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot (Checking for Heteroscedasticity)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nvdj8pKwbuL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It is Important to Address Heteroscedasticity**\n",
        "\n",
        "If heteroscedasticity is ignored, it can cause:\n",
        "* Incorrect standard errors - Leads to misleading hypothesis tests and incorrect p-values.\n",
        "* Inefficient regression estimates - OLS regression remains unbiased but is no longer the best linear unbiased estimator.\n",
        "* Poor predictive performance - Predictions may be unreliable, especially in real-world applications.\n",
        "\n",
        "**How to Fix Heteroscedasticity**\n",
        "* Use Robust Standard Errors - Use Heteroscedasticity-Consistent (HC) standard errors in regression models.\n",
        "* Transform the Dependent Variable (Y) - Apply log, square root, or Box-Cox transformations to stabilize variance.\n",
        "* Weighted Least Squares (WLS) - Assign weights to observations with higher variance to balance the influence.\n",
        "* Generalized Least Squares (GLS) - Adjusts for non-constant variance in errors.\n",
        "\n",
        "**Python code Using Robust Standard Errors**"
      ],
      "metadata": {
        "id": "IOYi6gfRbzLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "robust_model = model.get_robustcov_results()\n",
        "print(robust_model.summary())"
      ],
      "metadata": {
        "id": "eGPgEe1jcV7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "**Ans** - **High R² but Low Adjusted R² in Multiple Linear Regression**\n",
        "\n",
        "If a Multiple Linear Regression model has a high R² but a low Adjusted R², it usually indicates that the model includes unnecessary predictors that do not significantly contribute to explaining the dependent variable.\n",
        "\n",
        "1. Understanding R² vs. Adjusted R²\n",
        "* R² (Coefficient of Determination):\n",
        "  * Measures how much of the variance in Y is explained by the independent variables.\n",
        "  * Always increases when more predictors are added, even if they are irrelevant.\n",
        "* Adjusted R²:\n",
        "  * Adjusts for the number of predictors in the model.\n",
        "  * Penalizes the inclusion of unnecessary variables.\n",
        "  * Increases only if a new predictor improves model performance significantly.\n",
        "\n",
        "**Formula for Adjusted R²:**\n",
        "\n",
        "    Adjusted R² = 1 - ({(1−R²)(n−1)}/(n−k−1))\n",
        "where:\n",
        "* n = Number of observations\n",
        "* k = Number of predictors\n",
        "\n",
        "**2. What Causes High R² but Low Adjusted R²**\n",
        "* Multicollinearity: Some independent variables are highly correlated, making them redundant.\n",
        "* Too Many Irrelevant Predictors: The model is overfitting by adding variables that do not actually contribute to predicting Y.\n",
        "* Small Sample Size: With few observations, adding extra predictors artificially inflates R² but harms Adjusted R².\n",
        "\n",
        "**3. Example Interpretation**\n",
        "\n",
        "|Model |R² |Adjusted R² |Interpretation|\n",
        "|-||||\n",
        "|Model A(2 predictors)\t|0.78\t|0.76\t|Good model (predictors explain variance well)|\n",
        "|Model B(10 predictors)\t|0.85\t|0.40\t|Overfitted model (irrelevant predictors included)|\n",
        "\n",
        "* If Adjusted R² is much lower than R², some predictors are likely useless and should be removed.\n",
        "\n",
        "**4. How to Fix This Issue**\n",
        "* Remove Insignificant Predictors - Use p-values & feature selection techniques (e.g., Stepwise Regression, Lasso).\n",
        "* Check for Multicollinearity - Use Variance Inflation Factor (VIF) and drop redundant variables.\n",
        "* Use Cross-Validation - Evaluate the model on different datasets to check if additional predictors actually help.\n",
        "* Try Feature Engineering - Instead of adding more variables, derive meaningful features that better capture relationships."
      ],
      "metadata": {
        "id": "5xAn9hkGPCSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 22. Why is it important to scale variables in Multiple Linear Regression/\n",
        "**Ans** - **Importance of Scaling Variables in Multiple Linear Regression**\n",
        "\n",
        "In Multiple Linear Regression, scaling variables is important for:\n",
        "* Improving numerical stability\n",
        "* Handling different units and magnitudes\n",
        "* Enhancing interpretability in some cases\n",
        "\n",
        "**1. Scale Variables in Multiple Linear Regression**\n",
        "* Preventing Large Coefficients & Numerical Instability\n",
        "  * When independent variables have vastly different ranges, regression coefficients may become very large or small, leading to numerical instability.\n",
        "  * Example: If one variable is in millions and another in decimals, the model struggles to optimize efficiently.\n",
        "  * Fix: Scaling makes the optimization process smoother.\n",
        "* Avoiding Dominance of High-Magnitude Variables\n",
        "  * Unscaled features with large values tend to dominate the model, making smaller-valued features seem insignificant.\n",
        "  * Example:\n",
        "    * Salary (in ₹) ranges from 10,000 to 100,000\n",
        "    * Experience (years) ranges from 1 to 10\n",
        "    * Without scaling, the model might give more weight to Salary just because it has larger numbers.\n",
        "* Required for Regularization Techniques (Lasso & Ridge Regression)\n",
        "  * Lasso (L1) and Ridge (L2) regression penalize large coefficients.\n",
        "  * Without scaling, the penalty affects variables unequally.\n",
        "  * Fix: Scaling ensures all variables contribute fairly to the penalty term.\n",
        "* Improves Gradient Descent Convergence\n",
        "  * If using Gradient Descent, unscaled features slow down learning.\n",
        "  * Fix: Scaling ensures faster and more stable convergence.\n",
        "* Helps Interpret Regression Coefficients in Some Cases\n",
        "  * When variables are standardized (mean = 0, std = 1), regression coefficients represent how many standard deviations Y changes per standard deviation of X.\n",
        "  * Useful for comparing feature importance.\n",
        "\n",
        "**2. How to Scale Variables**\n",
        "* Standardization (Z-score Scaling):\n",
        "\n",
        "      X' = [{X-mean(X)}/std(X)]\n",
        "  * Used when features follow a normal distribution.\n",
        "  * Centers data around 0 with a standard deviation of 1.\n",
        "* Min-Max Scaling (Normalization):\n",
        "\n",
        "      X' = (X -Xₘᵢₙ)/(Xₘₐₓ - Xₘᵢₙ)\n",
        "  * Rescales values between 0 and 1.\n",
        "  * Useful when data has different units but no extreme outliers.\n",
        "\n",
        "**3. When Scaling is NOT Necessary**\n",
        "* When using Ordinary Least Squares (OLS) regression without regularization.\n",
        "* If all variables are already on a similar scale (e.g., percentages).\n",
        "* If regression coefficients’ magnitude is not important for interpretation.\n",
        "\n",
        "**4. Python Example: Scaling in Multiple Linear Regression**"
      ],
      "metadata": {
        "id": "VnlYic6kPCnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"Salary\": [10000, 25000, 50000, 75000, 100000],\n",
        "    \"Experience\": [1, 3, 5, 7, 10],\n",
        "    \"Performance_Score\": [2, 4, 5, 8, 9],\n",
        "    \"Y\": [30, 50, 75, 100, 130]\n",
        "})\n",
        "\n",
        "X = data[[\"Salary\", \"Experience\", \"Performance_Score\"]]\n",
        "y = data[\"Y\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "print(\"Coefficients after scaling:\", model.coef_)"
      ],
      "metadata": {
        "id": "jRiW-MtdiCZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 23. What is polynomial regression?\n",
        "**Ans** - **Polynomial Regression: An Overview**\n",
        "\n",
        "Polynomial Regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial. Unlike Simple Linear Regression, Polynomial Regression fits a curved line to the data.\n",
        "* Useful when the relationship between X and Y is non-linear but can be approximated using polynomial terms.\n",
        "\n",
        "**2. Polynomial Regression Equation**\n",
        "The general equation for Polynomial Regression is:\n",
        "\n",
        "        Y = b₀ + b₁X + b₂X² + b₃X³+...+bₙXⁿ + ϵ\n",
        "where:\n",
        "* b₀,b₁,b₂,...,bₙ are the regression coefficients\n",
        "* X,X²,X³,...,Xⁿ are polynomial terms (higher-order transformations of X).\n",
        "* ϵ is the error term.\n",
        "* The degree n determines the complexity of the model.\n",
        "\n",
        "**Example: A quadratic regression model (degree 2) is:**\n",
        "\n",
        "    Y = b₀ + b₁X + b₂X² + ϵ\n",
        "This fits a parabolic curve to the data.\n",
        "\n",
        "**3. Use Polynomial Regression**\n",
        "* Captures Non-Linearity - When data shows curvature, polynomial regression provides a better fit than linear regression.\n",
        "* More Flexible than Linear Models - Can fit a wider range of data patterns.\n",
        "* Simple to Implement - Uses the same principles as multiple linear regression but with transformed variables.\n",
        "\n",
        "**Limitations:**\n",
        "* Overfitting - High-degree polynomials can fit noise instead of true relationships.\n",
        "* Extrapolation Issues - Predictions outside the data range can be highly inaccurate.\n",
        "* Collinearity - Higher-degree terms can be highly correlated, leading to unstable estimates.\n",
        "\n",
        "**4. Python Example: Polynomial Regression**"
      ],
      "metadata": {
        "id": "5PkdL2pFPDif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n",
        "y = np.array([2.5, 4.5, 7.5, 11, 15.5, 21, 27.5, 35, 43.5])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial Fit (Degree=2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Polynomial Regression Example\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "atlhF_ej4a0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. When to Use Polynomial Regression**\n",
        "* If the data shows a curved pattern instead of a straight-line relationship.\n",
        "* When linear regression fails to capture complexity in the data.\n",
        "* For modeling trends in finance, physics, and engineering applications."
      ],
      "metadata": {
        "id": "Ou7zGSel4fYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 24. How does polynomial regression differ from linear regression?\n",
        "**Ans** - **Polynomial Regression vs. Linear Regression**\n",
        "\n",
        "Both Linear Regression and Polynomial Regression are used for modeling relationships between independent variables (X) and dependent variables (Y), but they differ in how they capture the relationship.\n",
        "\n",
        "**1. Differences**\n",
        "\n",
        "|Feature\t|Linear Regression\t|Polynomial Regression|\n",
        "|-|||\n",
        "|Equation |Y = b₀ + b₁X + ϵ |Y = b₀ + b₁X + b₂X² + b₃X³ +...+ bₙXⁿ + ϵ|\n",
        "|Nature of Relationship\t|Models straight-line relationships between X and Y.\t|Models curved (non-linear) relationships.|\n",
        "|Flexibility\t|Less flexible, only fits linear trends.\t|More flexible, captures non-linearity in data.|\n",
        "|Feature Transformation\t|Uses X as is.\t|Uses polynomial terms of X (e.g.,X²,X³, etc.).|\n",
        "|Overfitting Risk\t|Lower (unless too many variables).\t|Higher for large-degree polynomials.|\n",
        "|Interpretability\t|Easy to interpret coefficients.\t|Harder to interpret higher-degree terms.|\n",
        "\n",
        "**2. Visual Comparison**\n",
        "* Linear Regression Example (Degree = 1)\n",
        "  * Fits a straight line to the data.\n",
        "  * Works well when the relationship between X and Y is roughly linear.\n",
        "* Polynomial Regression Example (Degree ≥ 2)\n",
        "  * Fits a curved line that adapts to the data.\n",
        "  * Captures trends that linear regression fails to model.\n",
        "\n",
        "Example Visualization:\n",
        "* Linear Fit - Straight line\n",
        "* Polynomial Fit (Degree 2, 3, etc.) - Curved line\n",
        "\n",
        "**3. Python Example: Comparing Both**"
      ],
      "metadata": {
        "id": "1HtXdhwZPD3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n",
        "y = np.array([2.5, 4.5, 7.5, 11, 15.5, 21, 27.5, 35, 43.5])\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X, y)\n",
        "y_linear_pred = linear_model.predict(X)\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_poly, y)\n",
        "y_poly_pred = poly_model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(X, y_linear_pred, color='green', label=\"Linear Fit\")\n",
        "plt.plot(X, y_poly_pred, color='red', label=\"Polynomial Fit (Degree=2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Linear vs. Polynomial Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4k27abHH67o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. When to Use Each**\n",
        "* Use Linear Regression When:\n",
        "  * The relationship between X and Y is roughly linear.\n",
        "  * You prefer a simple, interpretable model.\n",
        "  * You want to avoid overfitting.\n",
        "* Use Polynomial Regression When:\n",
        "  * The relationship between X and Y shows curvature.\n",
        "  * You need a better fit for non-linear trends.\n",
        "  * You have enough data to avoid overfitting."
      ],
      "metadata": {
        "id": "lmcEfl4W7AL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 25. When is polynomial regression used?\n",
        "**Ans** - **When is Polynomial Regression Used**\n",
        "\n",
        "Polynomial regression is used when the relationship between the independent variable (X) and the dependent variable (Y) is non-linear and cannot be well-approximated by a straight line. Instead of a linear relationship, polynomial regression models curved trends using polynomial terms (X²,X³,Xⁿ).\n",
        "\n",
        "**Situations Where Polynomial Regression is Useful**\n",
        "1. When the Data Shows a Curved Relationship\n",
        "  * If plotting X vs. Y shows a clear curvature, polynomial regression provides a better fit than linear regression.\n",
        "  * Example: Predicting population growth, where early growth is slow, then accelerates, and later stabilizes.\n",
        "2. When a Simple Linear Model Fails to Capture Trends\n",
        "  * If residual plots from linear regression show a pattern, it suggests a non-linear relationship.\n",
        "  * Example: Stock price trends, where prices don't move in a straight line but have fluctuations.\n",
        "3. When You Want a Balance Between Simplicity & Flexibility\n",
        "  * A low-degree polynomial (X² or X³) can provide a better fit than a straight line while avoiding overfitting.\n",
        "  * Example: Modeling sales revenue based on ad spending, where small spending increases may have little effect, but higher spending has an exponential impact.\n",
        "4. Engineering & Physics Applications\n",
        "  * Many natural processes follow non-linear patterns.\n",
        "  * Examples:\n",
        "    * Projectile motion (Y = ax²+bx+c) in physics.\n",
        "    * Turbulent fluid flow in engineering.\n",
        "    * Growth of bacteria in biology.\n",
        "5. Machine Learning Feature Engineering\n",
        "  * Polynomial regression is often used to transform features in machine learning models.\n",
        "  * Helps linear models learn complex patterns without requiring deep learning techniques.\n",
        "  * Example: Polynomial features in support vector machines (SVMs) for classification tasks.\n",
        "\n",
        "**Real-World Examples of Polynomial Regression**\n",
        "  1. Economics & Finance:\n",
        "    * Stock Market Analysis - Predicting stock trends using quadratic or cubic polynomial regression.\n",
        "    * Consumer Demand Forecasting - Modeling demand curves where demand does not increase linearly with price changes.\n",
        "  2. Medicine & Biology:\n",
        "    * Dose-Response Curves - In pharmacology, the effect of a drug dose is often non-linear.\n",
        "    * Heart Rate & Exercise - The relationship between exercise intensity and heart rate follows a curved pattern.\n",
        "  3. Engineering & Physics:\n",
        "    * Vehicle Speed vs. Fuel Consumption - Fuel efficiency does not change linearly with speed.\n",
        "    * Projectile Motion - Follows a quadratic equation in physics.\n",
        "  4. Marketing & Sales:\n",
        "    * Advertising Spend vs. Sales Growth - A small increase in ad spend may not significantly impact sales, but a larger spend might.\n",
        "\n",
        "**When NOT to Use Polynomial Regression**\n",
        "  1. When Data is Truly Linear\n",
        "    * If the relationship is linear, using polynomial regression can lead to overfitting.\n",
        "  2. When Extrapolation is Needed\n",
        "    * Polynomial regression works well within the data range, but predictions outside the range can be highly inaccurate.\n",
        "  3. When There is Multicollinearity\n",
        "    * Higher-degree polynomial terms (X²,X³ etc.) can introduce multicollinearity, making coefficients unstable.\n",
        "  4. When Simplicity is Preferred\n",
        "    * Higher-degree polynomials reduce interpretability. If a simple linear model works well, it's usually preferable.\n",
        "\n",
        "**Choosing the Right Polynomial Degree**\n",
        "* Degree = 1 - Linear Regression (Straight Line)\n",
        "* Degree = 2 - Quadratic Regression (U-Shaped or Inverted U-Shaped Curve)\n",
        "* Degree = 3+ - Higher-Order Polynomials (More Complex Curves)\n",
        "* Use visualization and cross - validation to choose the optimal degree.\n",
        "* Too high a degree - Overfitting (fits noise rather than trends).\n",
        "* Too low a degree - Underfitting (fails to capture the pattern)."
      ],
      "metadata": {
        "id": "H5TnVI3GPEIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 26. What is the general equation for polynomial regression?\n",
        "**Ans** - **General Equation for Polynomial Regression**\n",
        "\n",
        "The general form of a Polynomial Regression equation is:\n",
        "\n",
        "    Y = b₀ + b₁X + b₂X² + b₃X³+...+bₙXⁿ + ϵ\n",
        "where:\n",
        "* Y = Dependent variable\n",
        "* X = Independent variable\n",
        "* b₀ = Intercept\n",
        "* b₁,b₂,b₃,...,bₙ = Regression coefficients\n",
        "* X²,X³,...,Xⁿ = Polynomial terms\n",
        "* n = Degree of the polynomial\n",
        "* ϵ = Error term\n",
        "\n",
        "**Examples of Different Degrees**\n",
        "1. Linear Regression (Degree 1)\n",
        "\n",
        "        Y = b₀ + b₁X + ϵ\n",
        "  - Fits a straight line\n",
        "\n",
        "2. Quadratic Regression (Degree 2)\n",
        "\n",
        "        Y = b₀ + b₁X + b₂X² + ϵ\n",
        "  - Fits a parabolic curve (U-shaped or inverted U)\n",
        "\n",
        "3. Cubic Regression (Degree 3)\n",
        "\n",
        "        Y = b₀ + b₁X + b₂X² + b₃X³ + ϵ\n",
        "  - Fits an S-shaped curve\n",
        "\n",
        "4. Higher-Degree Polynomial (Degreenn)\n",
        "\n",
        "        Y = b₀ + b₁X + b₂X² + b₃X³ +...+ bₙXⁿ + ϵ\n",
        "  - Fits complex curves"
      ],
      "metadata": {
        "id": "FcDOWDj7PEdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 27. Can polynomial regression be applied to multiple variables?\n",
        "**Ans** - Yes, Polynomial regression can be extended to multiple variables, which is known as Multivariable Polynomial Regression or Polynomial Multiple Regression. This means that instead of just one independent variable (X), the model includes multiple predictors (X₁,X₂,X₃,...) with polynomial terms.\n",
        "\n",
        "**General Equation for Multivariable Polynomial Regression**\n",
        "For two independent variables (X₁ and X₂), the equation is:\n",
        "\n",
        "    Y = b₀ + b₁X₁ + b₂X₂ + b₃X₁² + b₄X₂² + b₅X₁X₂ + ϵ\n",
        "For three independent variables (X₁,X₂,X₃), the equation is:\n",
        "\n",
        "    Y = b₀ + b₁X₁ + b₂X₂ + b₃X₃ + b₄X₁² + b₅X₂² + b₆X₃² + b₇X₁X₂ + b₈X₁X₃ + b₉X₂X₃ + ϵ\n",
        "\n",
        "**Differences from Simple Polynomial Regression:**\n",
        "* Includes multiple independent variables.\n",
        "* Contains interaction terms (e.g.,X₁X₂), which capture relationships between predictors.\n",
        "* Allows for higher-degree polynomial terms (e.g.X₁²,X₂²).\n",
        "\n",
        "**Example: Python Implementation**"
      ],
      "metadata": {
        "id": "YtXqTaeMPE1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
        "y = np.array([5, 8, 12, 18, 25, 35])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "print(\"Polynomial Features:\", poly.get_feature_names_out())\n",
        "\n",
        "plt.scatter(range(len(y)), y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(range(len(y_pred)), y_pred, color='red', linestyle=\"dashed\", label=\"Polynomial Fit\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Target Variable (Y)\")\n",
        "plt.title(\"Multivariable Polynomial Regression Example\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HNmiNzrkmaiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to Use Multivariable Polynomial Regression**\n",
        "* When multiple variables influence the outcome non-linearly (e.g., predicting housing prices using area, number of rooms, and location).\n",
        "* When interaction effects between variables are important (e.g., how temperature and humidity together affect energy consumption).\n",
        "* When a simple linear model is not capturing complex relationships in the data.\n",
        "\n",
        "**Challenges:**\n",
        "\n",
        "* Overfitting (too many polynomial terms can lead to excessive complexity).\n",
        "* Multicollinearity (higher-degree terms can be highly correlated).\n",
        "* Computational Complexity (as the number of variables and degree increases, the model grows exponentially)."
      ],
      "metadata": {
        "id": "_yYZDjCdmed6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 28. What are the limitations of polynomial regression?\n",
        "**Ans** - **Limitations of Polynomial Regression**\n",
        "\n",
        "While Polynomial Regression is useful for modeling non-linear relationships, it has several limitations that must be considered before using it.\n",
        "\n",
        "**1. Risk of Overfitting**\n",
        "* As the polynomial degree increases, the model fits the training data very well but may fail on new data.\n",
        "* High-degree polynomials tend to capture noise rather than the true pattern.\n",
        "* Example: A 10th-degree polynomial might fit every data point perfectly but generalize poorly to unseen data.\n",
        "* Solution: Use cross-validation and regularization to control overfitting.\n",
        "\n",
        "**2. Sensitive to Outliers**\n",
        "* Polynomial regression magnifies the effect of outliers, especially for high-degree polynomials.\n",
        "* Since higher-degree terms grow rapidly (Xⁿ), outliers have an exaggerated impact on the curve.\n",
        "* Solution: Use robust regression techniques or remove outliers if justified.\n",
        "\n",
        "**3. Extrapolation is Unreliable**\n",
        "* Polynomial regression works well within the range of observed data but performs poorly for predictions outside this range.\n",
        "* The curve can behave erratically when extrapolated beyond the training data.\n",
        "* Example: If a 4th-degree polynomial fits sales data from 2010-2020, using it to predict sales in 2050 may give nonsensical results.\n",
        "* Solution: Use domain knowledge and limit predictions within the data range.\n",
        "\n",
        "**4. Computational Complexity**\n",
        "* As the number of variables and polynomial degree increase, the number of features grows exponentially.\n",
        "* Example:\n",
        "  * For 2 variables (X₁,X₂), a 2nd-degree polynomial has 6 terms:\n",
        "        Y = b₀ + b₁X₁ + b₂X₂ + b₃X₁ 2 + b₄X₂2 + b₅X₁X₂\n",
        "  * For 10 variables, a 3rd-degree polynomial has 286 features!\n",
        "* Solution: Use feature selection or reduce the degree if performance degrades.\n",
        "\n",
        "**5. Multicollinearity Issues**\n",
        "* Higher-degree polynomial terms (e.g.,X²,X³ ) are often highly correlated with the original variable.\n",
        "* This leads to unstable coefficients and makes it harder to interpret the model.\n",
        "* Solution: Use Ridge or Lasso Regression to reduce multicollinearity.\n",
        "\n",
        "**6. Harder to Interpret**\n",
        "* Linear regression has straightforward interpretations (e.g., \"for every 1 unit increase inX,Y increases by b₁\").\n",
        "* Polynomial regression is more complex and harder to explain.\n",
        "* Example: A cubic term (X³) affects the prediction non-linearly, making it difficult to communicate results to non-technical audiences.\n",
        "* Solution: Use visualisations to aid interpretation or prefer lower-degree models.\n",
        "\n",
        "**7. Requires More Data for Higher Degrees**\n",
        "* A high-degree polynomial requires more data points to be reliable.\n",
        "* If you fit a 6th-degree polynomial on 10 data points, the model is likely overfitting.\n",
        "* Solution: Use a lower-degree polynomial unless there's strong evidence for complex patterns."
      ],
      "metadata": {
        "id": "nKyr-oXbPFHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "**Ans** - **Methods to Evaluate Model Fit When Selecting the Degree of a Polynomial**\n",
        "\n",
        "Choosing the right polynomial degree is crucial to balance between underfitting and overfitting. Several techniques can help assess model fit and determine the optimal polynomial degree.\n",
        "\n",
        "**1. R² (Coefficient of Determination)**\n",
        "* Measures how well the model explains the variance in the target variable.\n",
        "* Range: 0 ≤ R² ≤ 1 (Higher is better).\n",
        "* A higher-degree polynomial will always increase R², even if the model is overfitting.\n",
        "\n",
        "* Limitation: A high R² does not guarantee a good model (overfitting risk).\n",
        "\n",
        "**2. Adjusted R²**\n",
        "* Adjusted R² penalizes unnecessary complexity by considering the number of predictors.\n",
        "* Formula:\n",
        "      Adjusted R² = 1-[(1-R²)(n-1)]/(n-p-1)\n",
        "where:\n",
        "* n = number of observations\n",
        "* p = number of predictors\n",
        "\n",
        "**Use Case:** If Adjusted R² decreases when adding polynomial terms, the model may be too complex.\n",
        "\n",
        "**3. Root Mean Squared Error (RMSE) & Mean Absolute Error (MAE)**\n",
        "* RMSE: Measures the standard deviation of residuals (errors).\n",
        "      RMSE = √[1/n∑(Yₐ꜀ₜᵤₐₗ -Yₚᵣₑₔᵢ꜀ₜₑₔ)²]\n",
        "* MAE: Measures the average absolute difference between actual and predicted values.\n",
        "      MAE = 1/n ∑|Yₐ꜀ₜᵤₐₗ -Yₚᵣₑₔᵢ꜀ₜₑₔ|\n",
        "* Lower RMSE/MAE = Better fit.\n",
        "\n",
        "**Use Case:** Compare RMSE/MAE across polynomial degrees to find the best trade-off between bias and variance.\n",
        "\n",
        "**4. Cross-Validation (e.g., k-Fold CV)**\n",
        "* Splits data into k subsets and trains the model on k-1 subsets while testing on the remaining one.\n",
        "* Computes average RMSE across folds to evaluate performance.\n",
        "\n",
        "**Use Case:** Prevents overfitting by ensuring the model generalizes to unseen data.\n",
        "\n",
        "**5. Residual Plots**\n",
        "* Plots residuals vs. predicted values to check for randomness.\n",
        "* If patterns or trends appear, a different polynomial degree may be needed.\n",
        "\n",
        "**Use Case:** If residuals form a curve, a higher-degree polynomial may be needed. If they diverge wildly, the model may be overfitting.\n",
        "\n",
        "**6. AIC (Akaike Information Criterion) & BIC (Bayesian Information Criterion)**\n",
        "* AIC & BIC penalize complex models with too many parameters.\n",
        "* Lower AIC/BIC = Better model.\n",
        "* Formula:\n",
        "      AIC= -2ln(L)+2p\n",
        "      AIC= -2ln(L)+2p\n",
        "      BIC= -2ln(L)+pln(n)\n",
        "      BIC= -2ln(L)+pln(n)\n",
        "where:\n",
        "* L = likelihood function\n",
        "* p = number of parameters\n",
        "* n = number of observations\n",
        "\n",
        "**Use Case:** Helps select the simplest model that explains the data well.\n",
        "\n",
        "**7. Train vs. Test Error (Overfitting Check)**\n",
        "* Train Error: Error on the training data (always low for high-degree polynomials).\n",
        "* Test Error: Error on unseen data (should be low for a good model).\n",
        "* Overfitting Indicator: If train error is low but test error is high, the polynomial degree is too high.\n",
        "\n",
        "**Use Case:** Select the polynomial degree where train and test errors are closest.\n",
        "\n",
        "**Summary Table: Methods for Choosing Polynomial Degree**\n",
        "\n",
        "|Method\t|Goal\t|Use Case|\n",
        "|-|||\n",
        "|R²\t|Measures explained variance\t|Use for basic fit assessment (but not alone).|\n",
        "|Adjusted R²\t|Penalizes unnecessary complexity\t|Use when adding polynomial terms.|\n",
        "|RMSE & MAE\t|Measures prediction error\t|Compare across polynomial degrees.|\n",
        "|Cross-Validation\t|Ensures generalization to new data\t|Prevents overfitting.|\n",
        "|Residual Plots\t|Detects patterns in errors\t|Choose a degree that makes residuals random.|\n",
        "|AIC & BIC\t|Penalizes complex models\t|Select the simplest good-fitting model.|\n",
        "|Train vs. Test Error\t|Detects overfitting\t|Avoid overly complex polynomials.|"
      ],
      "metadata": {
        "id": "1DPDjlgbPF8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 30. Why is visualization important in polynomial regression?\n",
        "**Ans** - **Visualization is important in Polynomial Regression**\n",
        "\n",
        "Visualization plays a crucial role in polynomial regression because it helps in understanding model behavior, detecting issues, and improving model selection. Here's why visualization is essential:\n",
        "\n",
        "**1. Helps Identify Non-Linearity in Data**\n",
        "* Before applying polynomial regression, visualizing the data can reveal non-linear patterns.\n",
        "* If a linear model does not fit well, a polynomial curve may be needed.\n",
        "* Example: Scatter plot of data points vs. a linear fit. If the linear model fails to capture curvature, a polynomial regression might be more suitable.\n",
        "\n",
        "**2. Detects Overfitting & Underfitting**\n",
        "* Underfitting: A polynomial degree too low (e.g., linear) may not capture the trend.\n",
        "* Overfitting: A polynomial degree too high (e.g., 6th-degree) may fit noise rather than the real trend.\n",
        "* Plotting the model can show if it smoothly follows the data or creates unnecessary complexity.\n",
        "\n",
        "**Visualization Tip:**\n",
        "* Plot different polynomial degrees (1st, 3rd, 6th) to compare their fit.\n",
        "* Look for unnatural oscillations (sign of overfitting).\n",
        "\n",
        "**3. Residual Plots for Model Evaluation**\n",
        "* A good model should have random residuals without patterns.\n",
        "* If residuals form a curve, the polynomial degree might be too low.\n",
        "* If residuals diverge wildly, the model is too complex.\n",
        "\n",
        "**Visualization Tip:** Plot residuals vs. predicted values to check randomness.\n",
        "\n",
        "**4. Train vs. Test Fit to Check Generalization**\n",
        "* Overfitting models perform well on training data but poorly on test data.\n",
        "* Visualizing training and test predictions helps compare model behavior.\n",
        "\n",
        "**Visualization Tip:**\n",
        "* Plot train and test predictions to ensure the model generalizes well.\n",
        "* A huge gap between train and test predictions means overfitting.\n",
        "\n",
        "**5. Choose the Best Polynomial Degree**\n",
        "* Plotting polynomial fits of different degrees can help find the sweet spot between underfitting and overfitting.\n",
        "* Ideally, choose a polynomial that follows the data trend without excessive complexity.\n",
        "\n",
        "**Example:** Compare 2nd-degree, 3rd-degree, and 5th-degree polynomials visually to see which one fits best.\n",
        "\n",
        "**Example: Python Code for Polynomial Regression Visualization**"
      ],
      "metadata": {
        "id": "x4wO1AUGPGOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.linspace(1, 10, 20).reshape(-1, 1)\n",
        "y = 2 * X**2 - 5*X + 3 + np.random.normal(0, 5, size=(20, 1))\n",
        "\n",
        "degrees = [1, 2, 5]\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i, d in enumerate(degrees):\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "    X_range = np.linspace(1, 10, 100).reshape(-1, 1)\n",
        "    X_poly_range = poly.transform(X_range)\n",
        "    y_pred = model.predict(X_poly_range)\n",
        "\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.scatter(X, y, color='blue', label='Data')\n",
        "    plt.plot(X_range, y_pred, color='red', label=f'Degree {d}')\n",
        "    plt.title(f'Polynomial Degree {d}')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1tN8W1oQwFOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 31. How is polynomial regression implemented in Python?\n",
        "**Ans** - **Implementing Polynomial Regression in Python**\n",
        "\n",
        "Polynomial regression can be implemented in Python using Scikit-Learn. Below is a step-by-step guide:\n",
        "\n",
        "**Step 1: Import Required Libraries**"
      ],
      "metadata": {
        "id": "b3KNn4VgPGfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "8iljiEDXtAh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Generate Sample Non-Linear Data**"
      ],
      "metadata": {
        "id": "Of56JPUutBo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "X = np.linspace(1, 10, 20).reshape(-1, 1)\n",
        "y = 2 * X**2 - 5*X + 3 + np.random.normal(0, 5, size=(20, 1))\n",
        "\n",
        "plt.scatter(X, y, color='blue', label=\"Data\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Scatter Plot of Data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xnFc1vMUtNp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Apply Polynomial Regression**\n",
        "\n",
        "(A) Transform Input Features to Polynomial Form"
      ],
      "metadata": {
        "id": "LVXPqRhbtRR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)"
      ],
      "metadata": {
        "id": "vVS3as2DtZfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The 'PolynomialFeatures' function expands X into polynomial terms:\n",
        "  * For degree = 2, it transforms X into:\n",
        "        [1,X,x²]\n",
        "\n",
        "(B) Train the Model"
      ],
      "metadata": {
        "id": "P8OOzWCJtfD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)"
      ],
      "metadata": {
        "id": "EQFq7H5Qt8B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The model is trained using least squares to minimize the error.\n",
        "\n",
        "**Step 4: Make Predictions & Visualize Results**"
      ],
      "metadata": {
        "id": "hnssan5Ft_Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.linspace(1, 10, 100).reshape(-1, 1)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "y_pred = model.predict(X_test_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label=\"Data\")\n",
        "plt.plot(X_test, y_pred, color='red', label=\"Polynomial Fit (Degree=2)\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Polynomial Regression Fit\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LoWTnvoxuIAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Evaluate Model Performance**"
      ],
      "metadata": {
        "id": "6Tvp3zynuLcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred = model.predict(X_poly)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_train_pred))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "\n",
        "print(f\"Intercept: {model.intercept_[0]:.2f}\")\n",
        "print(f\"Coefficients: {model.coef_.flatten()[1:]}\")"
      ],
      "metadata": {
        "id": "R1X4z0nNuSQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimenting with Different Polynomial Degrees**\n",
        "\n",
        "To find the optimal polynomial degree:"
      ],
      "metadata": {
        "id": "MG6UBIEzuVwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for d in [1, 2, 3, 5]:\n",
        "    poly = PolynomialFeatures(degree=d)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "\n",
        "    plt.scatter(X, y, color='blue', label=\"Data\")\n",
        "    plt.plot(X_test, y_pred, label=f\"Degree {d}\")\n",
        "    plt.xlabel(\"X\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.title(f\"Polynomial Regression (Degree={d})\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NRdaj6RxueKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}